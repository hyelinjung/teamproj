from flask import Flask, request, jsonify
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# âœ… Flask ì•± ìƒì„±
app = Flask(__name__)

class ChatbotModel:
    _instance = None  # âœ… ì‹±ê¸€í†¤ ê°ì²´ ì €ì¥

    def __new__(cls):
        if cls._instance is None:
            print("ğŸ”¹ [INFO] Fine-Tuned ì±—ë´‡ ëª¨ë¸ ë¡œë”© ì¤‘...")
            cls._instance = super(ChatbotModel, cls).__new__(cls)
            cls._instance.load_model()
        return cls._instance

    def load_model(self):
        """Fine-Tuned ëª¨ë¸ ë¡œë“œ"""
        model_path = "./gemma-finetuned"
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(model_path).to("cuda" if torch.cuda.is_available() else "cpu")
        print("âœ… Fine-Tuned ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    def get_response(self, user_input):
        """ì±—ë´‡ ì‘ë‹µ ìƒì„±"""
        inputs = self.tokenizer(f"ì‚¬ìš©ì: {user_input}\nì±—ë´‡:", return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
        outputs = self.model.generate(**inputs, max_length=200)
        response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # âœ… "ì±—ë´‡:" ì´í›„ì˜ ì‘ë‹µë§Œ ì¶”ì¶œ
        if "ì±—ë´‡:" in response_text:
            response_text = response_text.split("ì±—ë´‡:")[1].strip()

        return response_text

# âœ… ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
chatbot = ChatbotModel()

@app.route("/chatbot", methods=["POST"])
def chatbot_api():
    """Flask API ì—”ë“œí¬ì¸íŠ¸"""
    data = request.json
    user_input = data.get("message", "")

    if not user_input:
        return jsonify({"error": "ì…ë ¥ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤."}), 400

    response = chatbot.get_response(user_input)
    return jsonify({"response": response})

if __name__ == "__main__":
    app.run(port=5100, debug=True)
