import pandas as pd
import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import Dataset
from torch.utils.data import DataLoader
from huggingface_hub import login

# âœ… Hugging Face ë¡œê·¸ì¸
huggingface_token = os.getenv("HUGGINGFACE_TOKEN")
login(token=huggingface_token)

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "google/gemma-2b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# âœ… Mac OSì—ì„œ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ ì„ íƒ
if torch.backends.mps.is_available():  # âœ… Apple M1/M2 GPU ì§€ì› (Metal Performance Shaders)
    device = torch.device("mps")
    print("ğŸ”¹ Using MPS (Apple Metal GPU)")
elif torch.cuda.is_available():  # âœ… NVIDIA GPU (Macì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš© ë¶ˆê°€ëŠ¥)
    device = torch.device("cuda")
    print("ğŸ”¹ Using CUDA (NVIDIA GPU)")
else:
    device = torch.device("cpu")
    print("ğŸ”¹ Using CPU")

model.to(device)

# âœ… CSV ë°ì´í„° ë¡œë“œ
try:
    df = pd.read_csv("guideChatbot.csv")

    print("ğŸ”¹ CSV ë°ì´í„° ë¡œë“œ ì™„ë£Œ! ë°ì´í„° ìƒ˜í”Œ:")
    print(df.head())

    # âœ… Dataset ë³€í™˜
    dataset = Dataset.from_pandas(df)

    # âœ… í† í¬ë‚˜ì´ì§• (labels ì¶”ê°€)
    def tokenize_function(examples):
        tokenized_inputs = tokenizer(
            examples["inputs"],
            padding=True,
            truncation=True,
            max_length=512
        )

        # âœ… labels ì¶”ê°€ (input_ids ë³µì‚¬)
        tokenized_inputs["labels"] = tokenized_inputs["input_ids"]

        return tokenized_inputs

    tokenized_datasets = dataset.map(tokenize_function, batched=True)

    # âœ… ë°ì´í„°ì…‹ì„ í•™ìŠµìš©(train)ê³¼ ê²€ì¦ìš©(eval)ìœ¼ë¡œ ë¶„ë¦¬
    split_data = tokenized_datasets.train_test_split(test_size=0.2)  # 80% í•™ìŠµ, 20% ê²€ì¦
    train_dataset = split_data["train"]
    eval_dataset = split_data["test"]

    print(f"ğŸ”¹ í•™ìŠµ ë°ì´í„°ì…‹ í¬ê¸°: {len(train_dataset)}")
    print(f"ğŸ”¹ ê²€ì¦ ë°ì´í„°ì…‹ í¬ê¸°: {len(eval_dataset)}")

    # âœ… PyTorch DataLoader ìƒì„±
    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    eval_dataloader = DataLoader(eval_dataset, batch_size=4)

    # âœ… ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

    # âœ… í•™ìŠµ ë£¨í”„
    num_epochs = 3
    model.train()
    for epoch in range(num_epochs):
        print(f"ğŸ”¹ Epoch {epoch+1}/{num_epochs} ì‹œì‘...")
        total_loss = 0

        for batch in train_dataloader:
            # âœ… ë¦¬ìŠ¤íŠ¸ë¥¼ PyTorch Tensorë¡œ ë³€í™˜ (torch.stack() ì‚¬ìš©)
            inputs = {
                k: torch.stack([torch.tensor(t, dtype=torch.long) for t in v]).to(device)
                for k, v in batch.items() if k in ["input_ids", "attention_mask", "labels"]
            }

            outputs = model(**inputs)
            loss = outputs.loss

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_dataloader)
        print(f"âœ… Epoch {epoch+1}/{num_epochs} ì™„ë£Œ, í‰ê·  ì†ì‹¤(loss): {avg_loss:.4f}")

    print("âœ… Fine-Tuning ì™„ë£Œ!")

    # âœ… ëª¨ë¸ ì €ì¥
    model.save_pretrained("./gemma-finetuned")
    tokenizer.save_pretrained("./gemma-finetuned")
    print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")

except Exception as e:
    print(f"âŒ í•™ìŠµ ì˜¤ë¥˜ ë°œìƒ: {e}")
